output_global_parent_dir: results
wandb_run_id: slurm
output_dir: onlinedpo_pythia2.8b_tldr6.9b
run_name: onlinedpo_pythia2.8b_tldr6.9b
dataset_name: vwxyzjn/summarize_from_feedback_tldr_3_filtered_oai_preprocessing_1706381144
dataset_test_split: validation
max_length: 512
## online dpo stuff
vllm: True
loss_type: sigmoid
## ppo stuff
total_episodes: 129024
response_length: 53
num_ppo_epochs: 1
num_mini_batches: 1 
learning_rate: 3.0e-6
per_device_train_batch_size: 4
gradient_accumulation_steps: 42
model_name_or_path: mnoukhov/pythia2.8b-sft-tldr 
sft_model_path: mnoukhov/pythia2.8b-sft-tldr 
reward_model_path: mnoukhov/pythia2.8b-rm-tldr6.9b
# local_rollout_forward_batch_size: 16
non_eos_penalty: True
stop_token: eos
# evaluation_strategy: "steps"
# eval_steps: 0.2
## save strategy
save_strategy: steps
save_steps: 0.25
hub_strategy: all_checkpoints
logging_steps: 100
num_sample_generations: 5
save_generations: True
# lora
# use_peft: True
# lora_target_modules: all-linear
